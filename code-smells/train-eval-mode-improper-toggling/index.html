<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Train Eval Mode Improper Toggling | DSLinter - Linter for Machine Learnig Application - Specific Code Smells</title>
<meta name=keywords content="api-specific,model training,error-prone">
<meta name=description content="Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to &ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it&rdquo;.">
<meta name=author content>
<link rel=canonical href=https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/dslinter/assets/css/stylesheet.min.548091f41dc92b4a213f8dc4a49e22545a96b7d1b4ae4ad73c2ab3a70e4e8ea1.css integrity="sha256-VICR9B3JK0ohP43EpJ4iVFqWt9G0rkrXPCqzpw5OjqE=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/dslinter/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.2">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript><meta property="og:title" content="Train Eval Mode Improper Toggling">
<meta property="og:description" content="Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to &ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it&rdquo;.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/"><meta property="article:section" content="Code Smells">
<meta property="og:site_name" content="DSLinter - Linter for Machine Learnig Application - Specific Code Smells">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Train Eval Mode Improper Toggling">
<meta name=twitter:description content="Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to &ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it&rdquo;.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Train Eval Mode Improper Toggling","item":"https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Train Eval Mode Improper Toggling","name":"Train Eval Mode Improper Toggling","description":"Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to \u0026ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it\u0026rdquo;.","keywords":["api-specific","model training","error-prone"],"articleBody":"Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to “have the training mode set as close as possible to the inference step to avoid forgetting to set it”.\nType API Specific\nExisting Stage Model Training\nEffect Error-prone\nExample ### PyTorch # Violated Code def train(model, optimizer, epoch, train_loader, validation_loader): model.train() # 👈👈👈 for batch_idx, (data, target) in experiment.batch_loop(iterable=train_loader): data, target = Variable(data), Variable(target) # Inference output = model(data) loss_t = F.nll_loss(output, target) # The iconic grad-back-step trio optimizer.zero_grad() loss_t.backward() optimizer.step() if batch_idx % args.log_interval == 0: train_loss = loss_t.item() train_accuracy = get_correct_count(output, target) * 100.0 / len(target) experiment.add_metric(LOSS_METRIC, train_loss) experiment.add_metric(ACC_METRIC, train_accuracy) print('Train Epoch: {}[{}/{}({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx, len(train_loader), 100. * batch_idx / len(train_loader), train_loss)) with experiment.validation(): val_loss, val_accuracy = test(model, validation_loader) # 👈👈👈 experiment.add_metric(LOSS_METRIC, val_loss) experiment.add_metric(ACC_METRIC, val_accuracy) def test(model, test_loader): model.eval() # Recommended Fix def train(model, optimizer, epoch, train_loader, validation_loader): for batch_idx, (data, target) in experiment.batch_loop(iterable=train_loader): model.train() # 👈👈👈 data, target = Variable(data), Variable(target) # Inference output = model(data) loss_t = F.nll_loss(output, target) # The iconic grad-back-step trio optimizer.zero_grad() loss_t.backward() optimizer.step() if batch_idx % args.log_interval == 0: train_loss = loss_t.item() train_accuracy = get_correct_count(output, target) * 100.0 / len(target) experiment.add_metric(LOSS_METRIC, train_loss) experiment.add_metric(ACC_METRIC, train_accuracy) print('Train Epoch: {}[{}/{}({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx, len(train_loader), 100. * batch_idx / len(train_loader), train_loss)) with experiment.validation(): val_loss, val_accuracy = test(model, validation_loader) # 👈👈👈 experiment.add_metric(LOSS_METRIC, val_loss) experiment.add_metric(ACC_METRIC, val_accuracy) def test(model, test_loader): model.eval() Source: Paper Grey Literature  https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037  GitHub Commit Stack Overflow Documentation ","wordCount":"274","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/"},"publisher":{"@type":"Organization","name":"DSLinter - Linter for Machine Learnig Application - Specific Code Smells","logo":{"@type":"ImageObject","url":"https://hynn01.github.io/dslinter/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://hynn01.github.io/dslinter/ accesskey=h title="DSLinter - Linter for Machine Learnig Application - Specific Code Smells (Alt + H)">DSLinter - Linter for Machine Learnig Application - Specific Code Smells</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://hynn01.github.io/dslinter/code-smells/ title="Code Smells">
<span>Code Smells</span>
</a>
</li>
<li>
<a href=https://hynn01.github.io/dslinter/ title=Survey>
<span>Survey</span>
</a>
</li>
<li>
<a href=https://hynn01.github.io/dslinter/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://hynn01.github.io/dslinter/>Home</a></div>
<h1 class=post-title>
Train Eval Mode Improper Toggling
</h1>
<div class=post-meta>
</div>
</header>
<div class=post-content><h3 id=description>Description<a hidden class=anchor aria-hidden=true href=#description>#</a></h3>
<p>In PyTorch, calling <code>.eval()</code> means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to &ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it&rdquo;.</p>
<h3 id=type>Type<a hidden class=anchor aria-hidden=true href=#type>#</a></h3>
<p>API Specific</p>
<h3 id=existing-stage>Existing Stage<a hidden class=anchor aria-hidden=true href=#existing-stage>#</a></h3>
<p>Model Training</p>
<h3 id=effect>Effect<a hidden class=anchor aria-hidden=true href=#effect>#</a></h3>
<p>Error-prone</p>
<h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#75715e>### PyTorch</span>

<span style=color:#75715e># Violated Code</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(model, optimizer, epoch, train_loader, validation_loader):
    model<span style=color:#f92672>.</span>train() <span style=color:#75715e># 👈👈👈</span>
    <span style=color:#66d9ef>for</span> batch_idx, (data, target) <span style=color:#f92672>in</span> experiment<span style=color:#f92672>.</span>batch_loop(iterable<span style=color:#f92672>=</span>train_loader):
        data, target <span style=color:#f92672>=</span> Variable(data), Variable(target)
        <span style=color:#75715e># Inference</span>
        output <span style=color:#f92672>=</span> model(data)
        loss_t <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>nll_loss(output, target)
        <span style=color:#75715e># The iconic grad-back-step trio</span>
        optimizer<span style=color:#f92672>.</span>zero_grad()
        loss_t<span style=color:#f92672>.</span>backward()
        optimizer<span style=color:#f92672>.</span>step()
        <span style=color:#66d9ef>if</span> batch_idx <span style=color:#f92672>%</span> args<span style=color:#f92672>.</span>log_interval <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            train_loss <span style=color:#f92672>=</span> loss_t<span style=color:#f92672>.</span>item()
            train_accuracy <span style=color:#f92672>=</span> get_correct_count(output, target) <span style=color:#f92672>*</span> <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>/</span> len(target)
            experiment<span style=color:#f92672>.</span>add_metric(LOSS_METRIC, train_loss)
            experiment<span style=color:#f92672>.</span>add_metric(ACC_METRIC, train_accuracy)
            print(<span style=color:#e6db74>&#39;Train Epoch: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> [</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{}</span><span style=color:#e6db74> (</span><span style=color:#e6db74>{:.0f}</span><span style=color:#e6db74>%)]</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>Loss: </span><span style=color:#e6db74>{:.6f}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(
                epoch, batch_idx, len(train_loader),
                <span style=color:#ae81ff>100.</span> <span style=color:#f92672>*</span> batch_idx <span style=color:#f92672>/</span> len(train_loader), train_loss))
            <span style=color:#66d9ef>with</span> experiment<span style=color:#f92672>.</span>validation():
                val_loss, val_accuracy <span style=color:#f92672>=</span> test(model, validation_loader) <span style=color:#75715e># 👈👈👈</span>
                experiment<span style=color:#f92672>.</span>add_metric(LOSS_METRIC, val_loss)
                experiment<span style=color:#f92672>.</span>add_metric(ACC_METRIC, val_accuracy)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test</span>(model, test_loader):
   model<span style=color:#f92672>.</span>eval()


<span style=color:#75715e># Recommended Fix</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(model, optimizer, epoch, train_loader, validation_loader):
    <span style=color:#66d9ef>for</span> batch_idx, (data, target) <span style=color:#f92672>in</span> experiment<span style=color:#f92672>.</span>batch_loop(iterable<span style=color:#f92672>=</span>train_loader):
        model<span style=color:#f92672>.</span>train() <span style=color:#75715e># 👈👈👈</span>
        data, target <span style=color:#f92672>=</span> Variable(data), Variable(target)
        <span style=color:#75715e># Inference</span>
        output <span style=color:#f92672>=</span> model(data)
        loss_t <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>nll_loss(output, target)
        <span style=color:#75715e># The iconic grad-back-step trio</span>
        optimizer<span style=color:#f92672>.</span>zero_grad()
        loss_t<span style=color:#f92672>.</span>backward()
        optimizer<span style=color:#f92672>.</span>step()
        <span style=color:#66d9ef>if</span> batch_idx <span style=color:#f92672>%</span> args<span style=color:#f92672>.</span>log_interval <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            train_loss <span style=color:#f92672>=</span> loss_t<span style=color:#f92672>.</span>item()
            train_accuracy <span style=color:#f92672>=</span> get_correct_count(output, target) <span style=color:#f92672>*</span> <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>/</span> len(target)
            experiment<span style=color:#f92672>.</span>add_metric(LOSS_METRIC, train_loss)
            experiment<span style=color:#f92672>.</span>add_metric(ACC_METRIC, train_accuracy)
            print(<span style=color:#e6db74>&#39;Train Epoch: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> [</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{}</span><span style=color:#e6db74> (</span><span style=color:#e6db74>{:.0f}</span><span style=color:#e6db74>%)]</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>Loss: </span><span style=color:#e6db74>{:.6f}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(
                epoch, batch_idx, len(train_loader),
                <span style=color:#ae81ff>100.</span> <span style=color:#f92672>*</span> batch_idx <span style=color:#f92672>/</span> len(train_loader), train_loss))
            <span style=color:#66d9ef>with</span> experiment<span style=color:#f92672>.</span>validation():
                val_loss, val_accuracy <span style=color:#f92672>=</span> test(model, validation_loader) <span style=color:#75715e># 👈👈👈</span>
                experiment<span style=color:#f92672>.</span>add_metric(LOSS_METRIC, val_loss)
                experiment<span style=color:#f92672>.</span>add_metric(ACC_METRIC, val_accuracy)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test</span>(model, test_loader):
   model<span style=color:#f92672>.</span>eval()

</code></pre></div><h3 id=source>Source:<a hidden class=anchor aria-hidden=true href=#source>#</a></h3>
<h4 id=paper>Paper<a hidden class=anchor aria-hidden=true href=#paper>#</a></h4>
<h4 id=grey-literature>Grey Literature<a hidden class=anchor aria-hidden=true href=#grey-literature>#</a></h4>
<ul>
<li><a href=https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037>https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037</a></li>
</ul>
<h4 id=github-commit>GitHub Commit<a hidden class=anchor aria-hidden=true href=#github-commit>#</a></h4>
<h4 id=stack-overflow>Stack Overflow<a hidden class=anchor aria-hidden=true href=#stack-overflow>#</a></h4>
<h4 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h4>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://hynn01.github.io/dslinter/tags/api-specific/>api-specific</a></li>
<li><a href=https://hynn01.github.io/dslinter/tags/model-training/>model training</a></li>
<li><a href=https://hynn01.github.io/dslinter/tags/error-prone/>error-prone</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://hynn01.github.io/dslinter/>DSLinter - Linter for Machine Learnig Application - Specific Code Smells</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>