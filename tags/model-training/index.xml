<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model training on DSLinter - Linter for Machine Learnig Application - Specific Code Smells</title>
    <link>https://hynn01.github.io/dslinter/tags/model-training/</link>
    <description>Recent content in model training on DSLinter - Linter for Machine Learnig Application - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://hynn01.github.io/dslinter/tags/model-training/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Excessive Hyperparameter</title>
      <link>https://hynn01.github.io/dslinter/code-smells/excessive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/excessive-hyperparameter/</guid>
      <description>Description Excessive hyperparameter precision is a potential risk for overtuning. Overtuning occurs if an overly high precision hyperparameter value allows the model to perform particularly well while values in close range of it do not. Further, the choice of such a hyperparameter might be uninterpretable to users, which leads to an untrustworthy result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example ### Scikit-Learn from sklearn.naive_bayes import MultinomialN # Violated Code mnb = MultinomialNB(alpha = 0.</description>
    </item>
    
    <item>
      <title>Counterintuitive Hyperparameter</title>
      <link>https://hynn01.github.io/dslinter/code-smells/counterintuitive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/counterintuitive-hyperparameter/</guid>
      <description>Description Counterintuitive hyperparameters will also cause bugs. There are four posts on StackOverflow discussing where the bug in the program is, and it turns out that a large learning rate causes bugs. This implies that the developer should check whether the hyperparameters stay in the normal range when developing ML applications.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # TensorFlow import tensorflow as tf # Violated Code optimizer = tf.</description>
    </item>
    
    <item>
      <title>Memory Not Freed</title>
      <link>https://hynn01.github.io/dslinter/code-smells/memory-not-freed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/memory-not-freed/</guid>
      <description>Description ML application training is memory-consuming, and thus, it is essential to free memory in time. Some APIs are provided to alleviate the run-out-of-memory issue in deep learning libraries. TensorFlow&amp;rsquo;s documentation notes that if the model is created in a loop, it is suggested to use clear\_session() in the loop. Meanwhile, the GitHub repository &amp;ldquo;Pytorch best practice&amp;rdquo; recommends using .detach() to detach the tensor whenever possible. We suggest developers check whether they use these APIs to free the memory whenever possible in their code.</description>
    </item>
    
    <item>
      <title>Deterministic Algorithm Not Used</title>
      <link>https://hynn01.github.io/dslinter/code-smells/deterministic-algorithm-not-used/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/deterministic-algorithm-not-used/</guid>
      <description>Description Some libraries provide APIs for developers to use the deterministic algorithm. Using deterministic algorithms is another effort that can be made to improve reproducibility. In PyTorch, it is suggested to set torch.use_deterministic_algorithms(True) when debugging. However, the application will perform slower if this option is set, so it is suggested not to use it in the deploy stage. Developers should be aware of this setting during the development process.
Type Generic</description>
    </item>
    
    <item>
      <title>Log Parameter Approaching Zero</title>
      <link>https://hynn01.github.io/dslinter/code-smells/log-parameter-approaching-zero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/log-parameter-approaching-zero/</guid>
      <description>Description Several posts on Stack Overflow talk about the bugs that are not easy to discover caused by the log parameter approaching zero. In this kind of program, the log function variable turns to zero and raises an error during the training process. However, the error&amp;rsquo;s stack trace did not directly point to the line of code that the bug exist. This kind of problem is not easy to debug and might take a long training time to find.</description>
    </item>
    
    <item>
      <title>Matrix Multiplication API Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/matrix-multiplication-api-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/matrix-multiplication-api-misused/</guid>
      <description>Description np.matmul() in Numpy library is more readable than np.dot() in the semantic way. Whilenp.dot() provides heterogeneous behaviors depending on the shape of the data, np.matmul() behaves in a consistent way. When the multiply operation is performed on two-dimension matrixes, two APIs give a same result. However, np.matmul() is preferred than np.dot() for its clear semantic.
Type API Specific
Existing Stage Model Training
Effect Readability
Example ### NumPy import numpy as np a = [[1, 0], [0, 1]] b = [[4, 1], [2, 2]] # Violated Code np.</description>
    </item>
    
    <item>
      <title>Initialization Order Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/initialization-order-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/initialization-order-misused/</guid>
      <description>Description The AdamOptimizer class in the TensorFlow creates additional variables named &amp;ldquo;slots&amp;rdquo;. The variables must be initialized before training the model. Therefore, if the developer call initialize_all_variables() before calling AdamOptimizer and does not call the initializer afterward, the variables created by AdamOptimizer will not be initialized and might cause an error.
Type API Specific
Existing Stage Model Training
Effect Error-prone
Example ### TensorFlow import tensorflow as tf # Violated Code init = tf.</description>
    </item>
    
  </channel>
</rss>
